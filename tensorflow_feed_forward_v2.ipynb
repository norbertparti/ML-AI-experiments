{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_feed_forward_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6PNvfb7eD/IJqD2peuvw0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/norbertparti/ML-AI-experiments/blob/master/tensorflow_feed_forward_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RFi4W0Sm-Ea",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c4be1575-4246-464d-db8f-f81603f26c58"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gd8GlHSInNty",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b07c91f9-3231-461e-dff0-6e5e8c03b551"
      },
      "source": [
        "# Configuration for Deep Feed Forward (Multi Layer Perceptron) model\n",
        "N_samples = 100\n",
        "\n",
        "input_size, hidden_layer_size, output_size = 2, 12, 1\n",
        "batch_size = 10\n",
        "epochs = 1000\n",
        "\n",
        "print(f'Layers: {input_size} => {hidden_layer_size} => {hidden_layer_size} => {output_size}')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layers: 2 => 12 => 12 => 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_U3tCYun8fF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "4d0f9687-d6c6-4e76-ff23-566ca85c854e"
      },
      "source": [
        "# Generate training data\n",
        "xs = tf.round(tf.random.uniform([N_samples, input_size], minval=1, maxval=100, dtype=tf.float32))\n",
        "\n",
        "ys = tf.reduce_sum(xs, 1)\n",
        "ys = tf.reshape(ys, (N_samples, 1))\n",
        "\n",
        "print(f'input-shape: {xs.shape}, output-shape: {ys.shape}\\n')\n",
        "print(f'1. item => x: {xs[0]}, y: {ys[0]}')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input-shape: (100, 2), output-shape: (100, 1)\n",
            "\n",
            "1. item => x: [79. 35.], y: [114.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HR4jZ6SopQ7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ee44a66d-ffe0-4bf3-c76c-3fdade95a3ae"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(12, activation='relu', kernel_initializer=tf.initializers.RandomUniform, bias_initializer=tf.initializers.Ones))\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu', kernel_initializer=tf.initializers.RandomUniform, bias_initializer=tf.initializers.Ones))\n",
        "model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "\n",
        "step = int(N_samples / batch_size)\n",
        "adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.01, name='Adam'\n",
        ")\n",
        "\n",
        "\n",
        "for epoch in range(300):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i in range(step):\n",
        "        start = i * batch_size\n",
        "        batch_x = xs[start:start+step]\n",
        "        batch_y = ys[start:start+step]\n",
        "\n",
        "        loss_fn = lambda: tf.keras.backend.mean(tf.keras.losses.mse(tf.cast(batch_y, tf.float32),model(batch_x)))\n",
        "        var_list_fn = lambda: model.trainable_weights\n",
        "\n",
        "        # In eager mode, simply call minimize to update the list of variables.\n",
        "        ad = adam.minimize(loss_fn, var_list_fn)\n",
        "\n",
        "\n",
        "    print('Epoch', epoch+1, 'completed out of', epochs, ', loss:', loss_fn().numpy())\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 completed out of 1000 , loss: 6380.331\n",
            "Epoch 2 completed out of 1000 , loss: 421.0446\n",
            "Epoch 3 completed out of 1000 , loss: 15.361307\n",
            "Epoch 4 completed out of 1000 , loss: 212.2284\n",
            "Epoch 5 completed out of 1000 , loss: 100.64775\n",
            "Epoch 6 completed out of 1000 , loss: 17.894989\n",
            "Epoch 7 completed out of 1000 , loss: 10.729943\n",
            "Epoch 8 completed out of 1000 , loss: 12.990202\n",
            "Epoch 9 completed out of 1000 , loss: 10.57435\n",
            "Epoch 10 completed out of 1000 , loss: 11.710574\n",
            "Epoch 11 completed out of 1000 , loss: 10.00705\n",
            "Epoch 12 completed out of 1000 , loss: 10.454763\n",
            "Epoch 13 completed out of 1000 , loss: 9.778453\n",
            "Epoch 14 completed out of 1000 , loss: 9.837891\n",
            "Epoch 15 completed out of 1000 , loss: 9.482923\n",
            "Epoch 16 completed out of 1000 , loss: 9.383366\n",
            "Epoch 17 completed out of 1000 , loss: 9.128047\n",
            "Epoch 18 completed out of 1000 , loss: 8.967491\n",
            "Epoch 19 completed out of 1000 , loss: 8.748937\n",
            "Epoch 20 completed out of 1000 , loss: 8.563474\n",
            "Epoch 21 completed out of 1000 , loss: 8.36042\n",
            "Epoch 22 completed out of 1000 , loss: 8.165974\n",
            "Epoch 23 completed out of 1000 , loss: 7.9696426\n",
            "Epoch 24 completed out of 1000 , loss: 7.7742677\n",
            "Epoch 25 completed out of 1000 , loss: 7.581277\n",
            "Epoch 26 completed out of 1000 , loss: 7.388697\n",
            "Epoch 27 completed out of 1000 , loss: 7.198664\n",
            "Epoch 28 completed out of 1000 , loss: 7.0101805\n",
            "Epoch 29 completed out of 1000 , loss: 6.8239317\n",
            "Epoch 30 completed out of 1000 , loss: 6.6399817\n",
            "Epoch 31 completed out of 1000 , loss: 6.4584017\n",
            "Epoch 32 completed out of 1000 , loss: 6.279419\n",
            "Epoch 33 completed out of 1000 , loss: 6.102999\n",
            "Epoch 34 completed out of 1000 , loss: 5.929387\n",
            "Epoch 35 completed out of 1000 , loss: 5.7586007\n",
            "Epoch 36 completed out of 1000 , loss: 5.5906706\n",
            "Epoch 37 completed out of 1000 , loss: 5.4257293\n",
            "Epoch 38 completed out of 1000 , loss: 5.26381\n",
            "Epoch 39 completed out of 1000 , loss: 5.1049223\n",
            "Epoch 40 completed out of 1000 , loss: 4.94911\n",
            "Epoch 41 completed out of 1000 , loss: 4.796449\n",
            "Epoch 42 completed out of 1000 , loss: 4.6468773\n",
            "Epoch 43 completed out of 1000 , loss: 4.5004745\n",
            "Epoch 44 completed out of 1000 , loss: 4.3572173\n",
            "Epoch 45 completed out of 1000 , loss: 4.21712\n",
            "Epoch 46 completed out of 1000 , loss: 4.080195\n",
            "Epoch 47 completed out of 1000 , loss: 3.9463894\n",
            "Epoch 48 completed out of 1000 , loss: 3.8157132\n",
            "Epoch 49 completed out of 1000 , loss: 3.6881447\n",
            "Epoch 50 completed out of 1000 , loss: 3.5636966\n",
            "Epoch 51 completed out of 1000 , loss: 3.4422715\n",
            "Epoch 52 completed out of 1000 , loss: 3.3239143\n",
            "Epoch 53 completed out of 1000 , loss: 3.208587\n",
            "Epoch 54 completed out of 1000 , loss: 3.0962377\n",
            "Epoch 55 completed out of 1000 , loss: 2.9868302\n",
            "Epoch 56 completed out of 1000 , loss: 2.880323\n",
            "Epoch 57 completed out of 1000 , loss: 2.7767234\n",
            "Epoch 58 completed out of 1000 , loss: 2.6759458\n",
            "Epoch 59 completed out of 1000 , loss: 2.5779603\n",
            "Epoch 60 completed out of 1000 , loss: 2.4827428\n",
            "Epoch 61 completed out of 1000 , loss: 2.390219\n",
            "Epoch 62 completed out of 1000 , loss: 2.3003778\n",
            "Epoch 63 completed out of 1000 , loss: 2.2131548\n",
            "Epoch 64 completed out of 1000 , loss: 2.1285253\n",
            "Epoch 65 completed out of 1000 , loss: 2.0463934\n",
            "Epoch 66 completed out of 1000 , loss: 1.9667835\n",
            "Epoch 67 completed out of 1000 , loss: 1.8895851\n",
            "Epoch 68 completed out of 1000 , loss: 1.8147701\n",
            "Epoch 69 completed out of 1000 , loss: 1.7423013\n",
            "Epoch 70 completed out of 1000 , loss: 1.672142\n",
            "Epoch 71 completed out of 1000 , loss: 1.6042168\n",
            "Epoch 72 completed out of 1000 , loss: 1.5384698\n",
            "Epoch 73 completed out of 1000 , loss: 1.474877\n",
            "Epoch 74 completed out of 1000 , loss: 1.4133799\n",
            "Epoch 75 completed out of 1000 , loss: 1.3539417\n",
            "Epoch 76 completed out of 1000 , loss: 1.2964877\n",
            "Epoch 77 completed out of 1000 , loss: 1.2409892\n",
            "Epoch 78 completed out of 1000 , loss: 1.1873888\n",
            "Epoch 79 completed out of 1000 , loss: 1.1356338\n",
            "Epoch 80 completed out of 1000 , loss: 1.0856926\n",
            "Epoch 81 completed out of 1000 , loss: 1.0375048\n",
            "Epoch 82 completed out of 1000 , loss: 0.99102175\n",
            "Epoch 83 completed out of 1000 , loss: 0.94620675\n",
            "Epoch 84 completed out of 1000 , loss: 0.9030137\n",
            "Epoch 85 completed out of 1000 , loss: 0.86138344\n",
            "Epoch 86 completed out of 1000 , loss: 0.82127\n",
            "Epoch 87 completed out of 1000 , loss: 0.7826439\n",
            "Epoch 88 completed out of 1000 , loss: 0.7454509\n",
            "Epoch 89 completed out of 1000 , loss: 0.7096505\n",
            "Epoch 90 completed out of 1000 , loss: 0.67519933\n",
            "Epoch 91 completed out of 1000 , loss: 0.6420563\n",
            "Epoch 92 completed out of 1000 , loss: 0.6101705\n",
            "Epoch 93 completed out of 1000 , loss: 0.5795131\n",
            "Epoch 94 completed out of 1000 , loss: 0.5500494\n",
            "Epoch 95 completed out of 1000 , loss: 0.52172893\n",
            "Epoch 96 completed out of 1000 , loss: 0.49451214\n",
            "Epoch 97 completed out of 1000 , loss: 0.46837926\n",
            "Epoch 98 completed out of 1000 , loss: 0.44328958\n",
            "Epoch 99 completed out of 1000 , loss: 0.41920456\n",
            "Epoch 100 completed out of 1000 , loss: 0.3961063\n",
            "Epoch 101 completed out of 1000 , loss: 0.37394792\n",
            "Epoch 102 completed out of 1000 , loss: 0.3527289\n",
            "Epoch 103 completed out of 1000 , loss: 0.33241415\n",
            "Epoch 104 completed out of 1000 , loss: 0.31296816\n",
            "Epoch 105 completed out of 1000 , loss: 0.2943874\n",
            "Epoch 106 completed out of 1000 , loss: 0.27664682\n",
            "Epoch 107 completed out of 1000 , loss: 0.25973904\n",
            "Epoch 108 completed out of 1000 , loss: 0.24364729\n",
            "Epoch 109 completed out of 1000 , loss: 0.22836247\n",
            "Epoch 110 completed out of 1000 , loss: 0.21386655\n",
            "Epoch 111 completed out of 1000 , loss: 0.2001626\n",
            "Epoch 112 completed out of 1000 , loss: 0.18723503\n",
            "Epoch 113 completed out of 1000 , loss: 0.17507721\n",
            "Epoch 114 completed out of 1000 , loss: 0.16367832\n",
            "Epoch 115 completed out of 1000 , loss: 0.15302935\n",
            "Epoch 116 completed out of 1000 , loss: 0.14310105\n",
            "Epoch 117 completed out of 1000 , loss: 0.1338855\n",
            "Epoch 118 completed out of 1000 , loss: 0.12534888\n",
            "Epoch 119 completed out of 1000 , loss: 0.117466494\n",
            "Epoch 120 completed out of 1000 , loss: 0.11019535\n",
            "Epoch 121 completed out of 1000 , loss: 0.103493296\n",
            "Epoch 122 completed out of 1000 , loss: 0.097316444\n",
            "Epoch 123 completed out of 1000 , loss: 0.09160637\n",
            "Epoch 124 completed out of 1000 , loss: 0.08631055\n",
            "Epoch 125 completed out of 1000 , loss: 0.081368014\n",
            "Epoch 126 completed out of 1000 , loss: 0.0767385\n",
            "Epoch 127 completed out of 1000 , loss: 0.072364226\n",
            "Epoch 128 completed out of 1000 , loss: 0.06820105\n",
            "Epoch 129 completed out of 1000 , loss: 0.06422369\n",
            "Epoch 130 completed out of 1000 , loss: 0.060398795\n",
            "Epoch 131 completed out of 1000 , loss: 0.056710713\n",
            "Epoch 132 completed out of 1000 , loss: 0.05315149\n",
            "Epoch 133 completed out of 1000 , loss: 0.04972001\n",
            "Epoch 134 completed out of 1000 , loss: 0.046419777\n",
            "Epoch 135 completed out of 1000 , loss: 0.04325377\n",
            "Epoch 136 completed out of 1000 , loss: 0.040234946\n",
            "Epoch 137 completed out of 1000 , loss: 0.037367422\n",
            "Epoch 138 completed out of 1000 , loss: 0.034658123\n",
            "Epoch 139 completed out of 1000 , loss: 0.032107573\n",
            "Epoch 140 completed out of 1000 , loss: 0.029719358\n",
            "Epoch 141 completed out of 1000 , loss: 0.027488302\n",
            "Epoch 142 completed out of 1000 , loss: 0.025410738\n",
            "Epoch 143 completed out of 1000 , loss: 0.023476979\n",
            "Epoch 144 completed out of 1000 , loss: 0.021684483\n",
            "Epoch 145 completed out of 1000 , loss: 0.020021725\n",
            "Epoch 146 completed out of 1000 , loss: 0.018480087\n",
            "Epoch 147 completed out of 1000 , loss: 0.017052194\n",
            "Epoch 148 completed out of 1000 , loss: 0.015727278\n",
            "Epoch 149 completed out of 1000 , loss: 0.014504406\n",
            "Epoch 150 completed out of 1000 , loss: 0.013370506\n",
            "Epoch 151 completed out of 1000 , loss: 0.01232061\n",
            "Epoch 152 completed out of 1000 , loss: 0.011348997\n",
            "Epoch 153 completed out of 1000 , loss: 0.010450025\n",
            "Epoch 154 completed out of 1000 , loss: 0.009618831\n",
            "Epoch 155 completed out of 1000 , loss: 0.008851074\n",
            "Epoch 156 completed out of 1000 , loss: 0.008139997\n",
            "Epoch 157 completed out of 1000 , loss: 0.007482992\n",
            "Epoch 158 completed out of 1000 , loss: 0.0068759583\n",
            "Epoch 159 completed out of 1000 , loss: 0.006315221\n",
            "Epoch 160 completed out of 1000 , loss: 0.0057977806\n",
            "Epoch 161 completed out of 1000 , loss: 0.005321025\n",
            "Epoch 162 completed out of 1000 , loss: 0.0048793857\n",
            "Epoch 163 completed out of 1000 , loss: 0.0044731176\n",
            "Epoch 164 completed out of 1000 , loss: 0.004099206\n",
            "Epoch 165 completed out of 1000 , loss: 0.003753845\n",
            "Epoch 166 completed out of 1000 , loss: 0.0034356862\n",
            "Epoch 167 completed out of 1000 , loss: 0.0031427697\n",
            "Epoch 168 completed out of 1000 , loss: 0.0028734072\n",
            "Epoch 169 completed out of 1000 , loss: 0.0026263162\n",
            "Epoch 170 completed out of 1000 , loss: 0.0023988257\n",
            "Epoch 171 completed out of 1000 , loss: 0.002189393\n",
            "Epoch 172 completed out of 1000 , loss: 0.0019975782\n",
            "Epoch 173 completed out of 1000 , loss: 0.0018212181\n",
            "Epoch 174 completed out of 1000 , loss: 0.0016596677\n",
            "Epoch 175 completed out of 1000 , loss: 0.0015115857\n",
            "Epoch 176 completed out of 1000 , loss: 0.0013759456\n",
            "Epoch 177 completed out of 1000 , loss: 0.0012514228\n",
            "Epoch 178 completed out of 1000 , loss: 0.0011376716\n",
            "Epoch 179 completed out of 1000 , loss: 0.0010337175\n",
            "Epoch 180 completed out of 1000 , loss: 0.0009388762\n",
            "Epoch 181 completed out of 1000 , loss: 0.0008518266\n",
            "Epoch 182 completed out of 1000 , loss: 0.0007726078\n",
            "Epoch 183 completed out of 1000 , loss: 0.0007002718\n",
            "Epoch 184 completed out of 1000 , loss: 0.0006342544\n",
            "Epoch 185 completed out of 1000 , loss: 0.00057419745\n",
            "Epoch 186 completed out of 1000 , loss: 0.0005192136\n",
            "Epoch 187 completed out of 1000 , loss: 0.00046952566\n",
            "Epoch 188 completed out of 1000 , loss: 0.00042437855\n",
            "Epoch 189 completed out of 1000 , loss: 0.00038311217\n",
            "Epoch 190 completed out of 1000 , loss: 0.00034566762\n",
            "Epoch 191 completed out of 1000 , loss: 0.00031167053\n",
            "Epoch 192 completed out of 1000 , loss: 0.0002808887\n",
            "Epoch 193 completed out of 1000 , loss: 0.0002528561\n",
            "Epoch 194 completed out of 1000 , loss: 0.00022768571\n",
            "Epoch 195 completed out of 1000 , loss: 0.00020483031\n",
            "Epoch 196 completed out of 1000 , loss: 0.00018406831\n",
            "Epoch 197 completed out of 1000 , loss: 0.00016525073\n",
            "Epoch 198 completed out of 1000 , loss: 0.00014840052\n",
            "Epoch 199 completed out of 1000 , loss: 0.0001330796\n",
            "Epoch 200 completed out of 1000 , loss: 0.000119243035\n",
            "Epoch 201 completed out of 1000 , loss: 0.00010684169\n",
            "Epoch 202 completed out of 1000 , loss: 9.566493e-05\n",
            "Epoch 203 completed out of 1000 , loss: 8.555361e-05\n",
            "Epoch 204 completed out of 1000 , loss: 7.651403e-05\n",
            "Epoch 205 completed out of 1000 , loss: 6.832962e-05\n",
            "Epoch 206 completed out of 1000 , loss: 6.096022e-05\n",
            "Epoch 207 completed out of 1000 , loss: 5.4368167e-05\n",
            "Epoch 208 completed out of 1000 , loss: 4.8466667e-05\n",
            "Epoch 209 completed out of 1000 , loss: 4.317736e-05\n",
            "Epoch 210 completed out of 1000 , loss: 3.842248e-05\n",
            "Epoch 211 completed out of 1000 , loss: 3.4190714e-05\n",
            "Epoch 212 completed out of 1000 , loss: 3.0373154e-05\n",
            "Epoch 213 completed out of 1000 , loss: 2.6966323e-05\n",
            "Epoch 214 completed out of 1000 , loss: 2.396677e-05\n",
            "Epoch 215 completed out of 1000 , loss: 2.1262873e-05\n",
            "Epoch 216 completed out of 1000 , loss: 1.8816349e-05\n",
            "Epoch 217 completed out of 1000 , loss: 1.6676608e-05\n",
            "Epoch 218 completed out of 1000 , loss: 1.479949e-05\n",
            "Epoch 219 completed out of 1000 , loss: 1.3015126e-05\n",
            "Epoch 220 completed out of 1000 , loss: 1.15477915e-05\n",
            "Epoch 221 completed out of 1000 , loss: 1.0193771e-05\n",
            "Epoch 222 completed out of 1000 , loss: 8.9616005e-06\n",
            "Epoch 223 completed out of 1000 , loss: 7.931213e-06\n",
            "Epoch 224 completed out of 1000 , loss: 6.95255e-06\n",
            "Epoch 225 completed out of 1000 , loss: 6.1406477e-06\n",
            "Epoch 226 completed out of 1000 , loss: 5.3797207e-06\n",
            "Epoch 227 completed out of 1000 , loss: 4.7563126e-06\n",
            "Epoch 228 completed out of 1000 , loss: 4.165445e-06\n",
            "Epoch 229 completed out of 1000 , loss: 3.6510567e-06\n",
            "Epoch 230 completed out of 1000 , loss: 3.1873599e-06\n",
            "Epoch 231 completed out of 1000 , loss: 2.81634e-06\n",
            "Epoch 232 completed out of 1000 , loss: 2.4488909e-06\n",
            "Epoch 233 completed out of 1000 , loss: 2.169471e-06\n",
            "Epoch 234 completed out of 1000 , loss: 1.8588092e-06\n",
            "Epoch 235 completed out of 1000 , loss: 1.651603e-06\n",
            "Epoch 236 completed out of 1000 , loss: 1.423982e-06\n",
            "Epoch 237 completed out of 1000 , loss: 1.2513992e-06\n",
            "Epoch 238 completed out of 1000 , loss: 1.0928604e-06\n",
            "Epoch 239 completed out of 1000 , loss: 9.469717e-07\n",
            "Epoch 240 completed out of 1000 , loss: 8.195624e-07\n",
            "Epoch 241 completed out of 1000 , loss: 7.2651744e-07\n",
            "Epoch 242 completed out of 1000 , loss: 6.096554e-07\n",
            "Epoch 243 completed out of 1000 , loss: 5.443726e-07\n",
            "Epoch 244 completed out of 1000 , loss: 4.5459166e-07\n",
            "Epoch 245 completed out of 1000 , loss: 4.055866e-07\n",
            "Epoch 246 completed out of 1000 , loss: 3.494628e-07\n",
            "Epoch 247 completed out of 1000 , loss: 3.0276132e-07\n",
            "Epoch 248 completed out of 1000 , loss: 2.5793125e-07\n",
            "Epoch 249 completed out of 1000 , loss: 2.2580352e-07\n",
            "Epoch 250 completed out of 1000 , loss: 1.925655e-07\n",
            "Epoch 251 completed out of 1000 , loss: 1.6896229e-07\n",
            "Epoch 252 completed out of 1000 , loss: 1.4604157e-07\n",
            "Epoch 253 completed out of 1000 , loss: 1.1951924e-07\n",
            "Epoch 254 completed out of 1000 , loss: 1.1391239e-07\n",
            "Epoch 255 completed out of 1000 , loss: 8.76229e-08\n",
            "Epoch 256 completed out of 1000 , loss: 8.1124014e-08\n",
            "Epoch 257 completed out of 1000 , loss: 6.5557835e-08\n",
            "Epoch 258 completed out of 1000 , loss: 5.652255e-08\n",
            "Epoch 259 completed out of 1000 , loss: 4.9432856e-08\n",
            "Epoch 260 completed out of 1000 , loss: 4.1967724e-08\n",
            "Epoch 261 completed out of 1000 , loss: 3.9252335e-08\n",
            "Epoch 262 completed out of 1000 , loss: 2.8639624e-08\n",
            "Epoch 263 completed out of 1000 , loss: 2.7017085e-08\n",
            "Epoch 264 completed out of 1000 , loss: 2.2847962e-08\n",
            "Epoch 265 completed out of 1000 , loss: 1.7177081e-08\n",
            "Epoch 266 completed out of 1000 , loss: 1.5548721e-08\n",
            "Epoch 267 completed out of 1000 , loss: 1.3165118e-08\n",
            "Epoch 268 completed out of 1000 , loss: 1.0378426e-08\n",
            "Epoch 269 completed out of 1000 , loss: 1.0125222e-08\n",
            "Epoch 270 completed out of 1000 , loss: 7.895869e-09\n",
            "Epoch 271 completed out of 1000 , loss: 6.299524e-09\n",
            "Epoch 272 completed out of 1000 , loss: 6.853952e-09\n",
            "Epoch 273 completed out of 1000 , loss: 4.6173225e-09\n",
            "Epoch 274 completed out of 1000 , loss: 3.2654497e-09\n",
            "Epoch 275 completed out of 1000 , loss: 5.477341e-09\n",
            "Epoch 276 completed out of 1000 , loss: 3.4589902e-09\n",
            "Epoch 277 completed out of 1000 , loss: 2.4243492e-09\n",
            "Epoch 278 completed out of 1000 , loss: 3.3149263e-09\n",
            "Epoch 279 completed out of 1000 , loss: 1.2674718e-09\n",
            "Epoch 280 completed out of 1000 , loss: 1.436274e-09\n",
            "Epoch 281 completed out of 1000 , loss: 1.1204975e-09\n",
            "Epoch 282 completed out of 1000 , loss: 9.575161e-10\n",
            "Epoch 283 completed out of 1000 , loss: 7.4796846e-10\n",
            "Epoch 284 completed out of 1000 , loss: 9.1386027e-10\n",
            "Epoch 285 completed out of 1000 , loss: 6.475602e-10\n",
            "Epoch 286 completed out of 1000 , loss: 4.4819898e-10\n",
            "Epoch 287 completed out of 1000 , loss: 4.132744e-10\n",
            "Epoch 288 completed out of 1000 , loss: 3.2305253e-10\n",
            "Epoch 289 completed out of 1000 , loss: 2.3428584e-10\n",
            "Epoch 290 completed out of 1000 , loss: 2.386514e-10\n",
            "Epoch 291 completed out of 1000 , loss: 1.877197e-10\n",
            "Epoch 292 completed out of 1000 , loss: 1.0186341e-10\n",
            "Epoch 293 completed out of 1000 , loss: 1.8480932e-10\n",
            "Epoch 294 completed out of 1000 , loss: 3.8708095e-10\n",
            "Epoch 295 completed out of 1000 , loss: 1.7607818e-10\n",
            "Epoch 296 completed out of 1000 , loss: 1.9499566e-10\n",
            "Epoch 297 completed out of 1000 , loss: 2.939487e-10\n",
            "Epoch 298 completed out of 1000 , loss: 1.1350494e-10\n",
            "Epoch 299 completed out of 1000 , loss: 2.2846507e-10\n",
            "Epoch 300 completed out of 1000 , loss: 1.891749e-10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8EZaYQDpO_H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "875517ef-febb-4dc8-ceae-477977025578"
      },
      "source": [
        "import numpy as np\n",
        "model.predict(np.array([[5.0,6.0]]))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11.000013]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}